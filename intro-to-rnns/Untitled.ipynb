{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "with open(\"anna.txt\", 'r') as fin:\n",
    "    f = fin.read()\n",
    "f_set = set(f)\n",
    "print (len(f_set))\n",
    "vocab2id, id2vocab = {}, {}\n",
    "vocab2id = {w : i for i, w in enumerate(f_set)}\n",
    "id2vocab = dict(enumerate(f_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (f[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = np.array([vocab2id[w] for w in f], dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46 72 69 22 42 77  4 29 56 28]\n"
     ]
    }
   ],
   "source": [
    "print (encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(input_data, batch_size, seq_step, num_classes):\n",
    "    totalc = batch_size * seq_step\n",
    "    all_batch = len(input_data) // totalc\n",
    "    indata_len = all_batch * totalc\n",
    "    indata = input_data[0:indata_len]\n",
    "    for i in range(all_batch):\n",
    "        batch_data = indata[i:i+totalc]\n",
    "        target_data = indata[i+1:i+1+totalc]\n",
    "        nbatch_data = np.reshape(batch_data, [batch_size, seq_step])\n",
    "        ntarget_data = np.reshape(batch_data, [batch_size, seq_step])\n",
    "        \n",
    "        y = np.zeros(nbatch_data.shape, dtype = nbatch_data.dtype)\n",
    "        y[:, :ntarget_data.shape[1]] = ntarget_data\n",
    "        \n",
    "        nbatch = tf.one_hot(nbatch_data, num_classes).eval()\n",
    "        y = tf.one_hot(y, num_classes).eval()\n",
    "        #print (\"sss\")\n",
    "        #print (nbatch)\n",
    "        #print (y)\n",
    "        yield nbatch, y\n",
    "\n",
    "num_classes = len(f_set)\n",
    "batch_size = 10\n",
    "seq_step = 50\n",
    "a = get_batch(encoded, batch_size, seq_step, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input(batch_size, seq_step, ndim=0):\n",
    "    if ndim==0:\n",
    "        input_data = tf.placeholder(tf.float32, [batch_size, seq_step], name=\"input_data\")\n",
    "        target_data = tf.placeholder(tf.float32, [batch_size, seq_step], name=\"output_data\")\n",
    "    else:\n",
    "        input_data = tf.placeholder(tf.float32, [batch_size, seq_step, ndim], name=\"input_data\")\n",
    "        target_data = tf.placeholder(tf.float32, [batch_size, seq_step, ndim], name=\"output_data\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name= \"keep_prob\")\n",
    "    \n",
    "    return input_data, target_data, keep_prob\n",
    "\n",
    "build_input(10, 50, 83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(num_layer, lstm_size, batch_size, keep_prob):\n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop_cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob = keep_prob)\n",
    "        return drop_cell\n",
    "    lstm_cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layer)])\n",
    "    initial_state = lstm_cell.zero_state(batch_size, dtype= tf.float32) ##?????\n",
    "    return lstm_cell, initial_state\n",
    "build_lstm(3, 100, 10, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_final_output(outputs, lstm_size, outsize):\n",
    "    x = tf.reshape(outputs, [-1, lstm_size])\n",
    "    with tf.variable_scope(\"softmax\"):\n",
    "        w = tf.Variable(tf.truncated_normal([lstm_size, outsize], stddev = 0.1))\n",
    "        b = tf.Variable(tf.zeros(outsize))\n",
    "    logits  = tf.matmul(x, w) + b\n",
    "    out = tf.nn.softmax(logits, name= \"pred\")\n",
    "    \n",
    "    return logits, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):##??\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()  ##????\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, target):\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = target)\n",
    "    loss = tf.reduce_mean(loss)  ##?why mean, not sum\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##build struc\n",
    "class charRNN():\n",
    "    ##?grad_clip?????\n",
    "    def __init__(self, batch_size, seq_step, num_classes, num_layer=3, lstm_size=100, keep_prob=0.9, learning_rate=0.1, grad_clip=5):\n",
    "        tf.reset_default_graph()\n",
    "        lstm_cell, initial_state = build_lstm(num_layer, lstm_size, batch_size, keep_prob)\n",
    "        self.input_data, self.output_data, self.keep_prob = build_input(batch_size, seq_step, 0)\n",
    "        self.input_data2, self.output_data2, self.keep_prob2 = build_input(batch_size, seq_step, num_classes)\n",
    "        #print (self.input_data)\n",
    "        #print (self.input_data2)\n",
    "        #self.input_data1 = tf.one_hot(self.input_data, dim, dtype=tf.float32)\n",
    "        #print (self.input_data1)\n",
    "        #print (self.input_data2)\n",
    "        outputs, states = tf.nn.dynamic_rnn(lstm_cell, self.input_data2, initial_state=initial_state, scope=\"rnntest\")\n",
    "        logits, out = build_final_output(outputs, lstm_size, num_classes)\n",
    "        self.loss = build_loss(logits, self.output_data2)\n",
    "        self.opt = build_optimizer(self.loss, learning_rate, grad_clip)\n",
    "#charRNN(10, 50, 3, 100, 0.9, 83)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epchos = 10\n",
    "batch_size = 10\n",
    "seq_step = 50\n",
    "num_classes = len(f_set)\n",
    "keep_prob = 0.8\n",
    "\n",
    "model = charRNN(batch_size, seq_step, num_classes)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epchos):\n",
    "        for x, y in get_batch(encoded, batch_size, seq_step, num_classes):\n",
    "            print (x)\n",
    "            print (y)\n",
    "            feed = {model.input_data2:x, model.output_data2:y, model.keep_prob2:keep_prob}\n",
    "            loss, opt = sess.run([model.loss, model.opt], feed_dict=feed)\n",
    "            sys.exit()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
